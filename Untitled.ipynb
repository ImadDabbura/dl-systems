{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de500e0-a311-4d9b-9c8a-bda5ddbf3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7cc17-a9ff-4f8f-9b7a-e275d0fec718",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LayerNorm("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236d3867-e73a-45b4-9d33-631d94c27c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7302ce26-4a51-4b23-8aa5-beca64aa6acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mDocstring:\u001b[39m\n",
       "permute(*dims) -> Tensor\n",
       "\n",
       "See :func:`torch.permute`\n",
       "\u001b[31mType:\u001b[39m      method_descriptor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.Tensor.permute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd4b474-45f1-49da-818b-42691476be55",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 1"
     ]
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3], [4, 5, 6]]).permute(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b100720e-4cc9-4ab4-bcbc-9fc3a377d60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10, 5)[[1, 2, 3, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bb329c8-dbab-4d9f-aafd-f96649b66640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 6, 7, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ascontiguousarray(np.arange(9).reshape(3, 3)[::2]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c987da36-deeb-4e8d-b37d-3c97bfb40710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (None, 2)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip_longest(range(2), range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96feebeb-9a2b-4d66-b4c7-25acbba59339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53703373])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1bb5667-83b7-4c97-9889-0a6cfbb61c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[82, 81, 20, 99],\n",
       "       [36, 49, 84, 20],\n",
       "       [77, 74, 23, 19],\n",
       "       [33, 93, 87, 86],\n",
       "       [17, 29, 67, 79]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(low=1, high=100, size=(5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb855b15-5008-487c-958a-30875add7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "locals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e1884e39-7870-401f-a9f1-cb4333cf6fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "78ff6278-b36c-4c3b-99e4-12626b74ac56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "BatchSampler(\n",
       "    sampler: Union[torch.utils.data.sampler.Sampler[int], Iterable[int]],\n",
       "    batch_size: int,\n",
       "    drop_last: bool,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Wraps another sampler to yield a mini-batch of indices.\n",
       "\n",
       "Args:\n",
       "    sampler (Sampler or Iterable): Base sampler. Can be any iterable object\n",
       "    batch_size (int): Size of mini-batch.\n",
       "    drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
       "        its size would be less than ``batch_size``\n",
       "\n",
       "Example:\n",
       "    >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
       "    [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
       "    >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
       "    [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
       "\u001b[31mFile:\u001b[39m           ~/dev/dl-venv/lib/python3.13/site-packages/torch/utils/data/sampler.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BatchSampler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "29b8b4d5-7514-497a-b029-7bd30b310782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 5)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = zip(*[(1, 2), (3, 4), (5, 6)])\n",
    "a#, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ae2ca4d1-8799-4718-b887-b83119fb85ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mDocstring:\u001b[39m\n",
       "stack(tensors, dim=0, *, out=None) -> Tensor\n",
       "\n",
       "Concatenates a sequence of tensors along a new dimension.\n",
       "\n",
       "All tensors need to be of the same size.\n",
       "\n",
       ".. seealso::\n",
       "\n",
       "    :func:`torch.cat` concatenates the given sequence along an existing dimension.\n",
       "\n",
       "Arguments:\n",
       "    tensors (sequence of Tensors): sequence of tensors to concatenate\n",
       "    dim (int, optional): dimension to insert. Has to be between 0 and the number\n",
       "        of dimensions of concatenated tensors (inclusive). Default: 0\n",
       "\n",
       "Keyword args:\n",
       "    out (Tensor, optional): the output tensor.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> x = torch.randn(2, 3)\n",
       "    >>> x\n",
       "    tensor([[ 0.3367,  0.1288,  0.2345],\n",
       "            [ 0.2303, -1.1229, -0.1863]])\n",
       "    >>> torch.stack((x, x)) # same as torch.stack((x, x), dim=0)\n",
       "    tensor([[[ 0.3367,  0.1288,  0.2345],\n",
       "             [ 0.2303, -1.1229, -0.1863]],\n",
       "\n",
       "            [[ 0.3367,  0.1288,  0.2345],\n",
       "             [ 0.2303, -1.1229, -0.1863]]])\n",
       "    >>> torch.stack((x, x)).size()\n",
       "    torch.Size([2, 2, 3])\n",
       "    >>> torch.stack((x, x), dim=1)\n",
       "    tensor([[[ 0.3367,  0.1288,  0.2345],\n",
       "             [ 0.3367,  0.1288,  0.2345]],\n",
       "\n",
       "            [[ 0.2303, -1.1229, -0.1863],\n",
       "             [ 0.2303, -1.1229, -0.1863]]])\n",
       "    >>> torch.stack((x, x), dim=2)\n",
       "    tensor([[[ 0.3367,  0.3367],\n",
       "             [ 0.1288,  0.1288],\n",
       "             [ 0.2345,  0.2345]],\n",
       "\n",
       "            [[ 0.2303,  0.2303],\n",
       "             [-1.1229, -1.1229],\n",
       "             [-0.1863, -0.1863]]])\n",
       "    >>> torch.stack((x, x), dim=-1)\n",
       "    tensor([[[ 0.3367,  0.3367],\n",
       "             [ 0.1288,  0.1288],\n",
       "             [ 0.2345,  0.2345]],\n",
       "\n",
       "            [[ 0.2303,  0.2303],\n",
       "             [-1.1229, -1.1229],\n",
       "             [-0.1863, -0.1863]]])\n",
       "\u001b[31mType:\u001b[39m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.stack??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6a9cef33-9da5-45ae-ad9e-218054cdeb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(tuple(torch.arange(10) for i in range(5))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "003dcc21-3161-4a26-b0c6-a27ad54c9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f336d5f7-f107-4bb8-93fd-dc2aac8a4f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[81., 53., 15., 61.],\n",
       "         [29., 40.,  8., 37.],\n",
       "         [32.,  6.,  2., 50.],\n",
       "         [35., 21., 82., 70.],\n",
       "         [69., 10., 54., 59.]], requires_grad=True),\n",
       " tensor([[2., 0., 3., 2.],\n",
       "         [4., 0., 0., 4.],\n",
       "         [1., 1., 4., 3.],\n",
       "         [4., 3., 3., 3.],\n",
       "         [3., 2., 0., 1.]], requires_grad=True))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(high=100, size=(5, 4)).float().requires_grad_()\n",
    "y = torch.randint(high=5, size=(5, 4)).float().requires_grad_()\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b56e6956-3662-4498-95fa-073593c9f287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.5610e+03, 1.0000e+00, 3.3750e+03, 3.7210e+03],\n",
       "        [7.0728e+05, 1.0000e+00, 1.0000e+00, 1.8742e+06],\n",
       "        [3.2000e+01, 6.0000e+00, 1.6000e+01, 1.2500e+05],\n",
       "        [1.5006e+06, 9.2610e+03, 5.5137e+05, 3.4300e+05],\n",
       "        [3.2851e+05, 1.0000e+02, 1.0000e+00, 5.9000e+01]],\n",
       "       grad_fn=<PowBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x ** y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f92c62b2-e36f-47b2-b6ad-a556aa4158e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c01199b0-46d1-4622-a3ac-363f4fb4df36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.6200e+02, 0.0000e+00, 6.7500e+02, 1.2200e+02],\n",
       "         [9.7556e+04, 0.0000e+00, 0.0000e+00, 2.0261e+05],\n",
       "         [1.0000e+00, 1.0000e+00, 3.2000e+01, 7.5000e+03],\n",
       "         [1.7150e+05, 1.3230e+03, 2.0172e+04, 1.4700e+04],\n",
       "         [1.4283e+04, 2.0000e+01, 0.0000e+00, 1.0000e+00]]),\n",
       " tensor([[2.8832e+04, 3.9703e+00, 9.1397e+03, 1.5297e+04],\n",
       "         [2.3816e+06, 3.6889e+00, 2.0794e+00, 6.7674e+06],\n",
       "         [1.1090e+02, 1.0751e+01, 1.1090e+01, 4.8900e+05],\n",
       "         [5.3352e+06, 2.8195e+04, 2.4297e+06, 1.4572e+06],\n",
       "         [1.3909e+06, 2.3026e+02, 3.9890e+00, 2.4057e+02]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b28f642d-a00d-4db3-b265-406053245e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.27721421,   1.37059901,  -3.33208802,   3.10890114],\n",
       "       [  0.73830661,   2.37725191,   0.44184238,  -0.79613774],\n",
       "       [ -5.8293331 , -14.51917431,  -1.52112146,   0.48439182],\n",
       "       [  1.89638854,   1.97112056,  -2.68193949,  -1.00982751]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[ 0.78295402,  0.729608  , -0.30011212,  0.32165706],\n",
    "       [ 1.35445083,  0.42065378,  2.26325052, -1.25606406],\n",
    "       [-0.17154621, -0.06887444, -0.6574097 ,  2.06444442],\n",
    "       [ 0.5273181 ,  0.50732564, -0.37286449, -0.99026813]]) ** np.array([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9d8b66-5987-446c-b888-e3d4cc610f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47666174])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.4731075660869842]) ** 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4c374aa-a496-4bf0-86ff-9c923d105d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30., 96., 48., 75., 21.],\n",
       "       [ 8., 61., 47.,  2., 34.],\n",
       "       [89., 47., 17., 56., 83.],\n",
       "       [22., 29., 29., 73., 37.],\n",
       "       [ 4., 42.,  7., 28., 48.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(size=(5,5), low=0, high=100).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c7c2c2-889a-48be-a6a9-925a439d3ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "tupl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2acb425-c1a1-46f3-9f54-3b1051619b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(x for x in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b0a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dc3ec9f-79cf-4b01-90d7-33d460437c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, requires_grad=True).data.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "591e13fe-dabb-486f-861e-ab030898f4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hn/9dlqvjvx427_3m7wj0vxmpx80000gn/T/ipykernel_81762/3214292881.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2)\n",
    "y = torch.tensor(x)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06cfa00d-72c8-4a37-8826-de27dcdd6f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([101., 101.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add_(100)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68c3b398-e6df-41fb-8b45-09e5342ba925",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "y = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73983e04-cfd9-4e1b-abce-23bb9ba16d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 100\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c91d317-1f45-4c94-b164-38684ccad720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, requires_grad=True).detach().requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d218d9-d659-4c8a-b4d5-1f7f5a5cfc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([20, 10]), torch.Size([20])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Linear(10, 20)\n",
    "[p.shape for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "267c2156-339f-4eca-8aee-446344a3cea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0541, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> # NLP Example\n",
    ">>> batch, sentence_length, embedding_dim = 20, 5, 10\n",
    ">>> embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    ">>> layer_norm = nn.LayerNorm(embedding_dim)\n",
    ">>> # Activate module\n",
    ">>> layer_norm(embedding).std(dim=-1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf65d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_reference(Z, w):\n",
    "    # NHWC -> NCHW\n",
    "    Z_torch = torch.tensor(Z).permute(0, 3, 1, 2)\n",
    "\n",
    "    # KKIO -> OIKK\n",
    "    w_torch = torch.tensor(w).permute(3, 2, 0, 1)\n",
    "\n",
    "    out = f.conv2d(Z_torch, w_torch)\n",
    "\n",
    "    return out.permute(0, 2, 3, 1).contiguous().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e748d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "C_IN = 8\n",
    "C_OUT = 16\n",
    "H = W = 32\n",
    "K = 3\n",
    "Z = np.random.randn(N, H, W, C_IN)\n",
    "w = np.random.randn(K, K, C_IN, C_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44554cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 30, 16)\n"
     ]
    }
   ],
   "source": [
    "out = conv_reference(Z, w)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eece938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 ms ± 281 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit conv_reference(Z, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e9c38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_height(h, k, padding, stride):\n",
    "    return (h + 2 * padding - k) // stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "335c27a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_height(32, 3, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "449b4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_naive(Z, w):\n",
    "    N, H, W, C_IN = Z.shape\n",
    "    K, _, _, C_OUT = w.shape\n",
    "    new_h = new_w = get_height(H, K, 0, 1)\n",
    "    out = np.zeros((N, new_h, new_w, C_OUT))\n",
    "    for n in range(N):\n",
    "        for c_in in range(C_IN):\n",
    "            for c_out in range(C_OUT):\n",
    "                for y in range(new_h):\n",
    "                    for x in range(new_w):\n",
    "                        for i in range(K):\n",
    "                            for j in range(K):\n",
    "                                out[n, y, x, c_out] += Z[n, y + i, x + j, c_in] * w[i, j, c_in, c_out]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a64704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.144325265762093e-12\n"
     ]
    }
   ],
   "source": [
    "out2 = conv_naive(Z, w)\n",
    "print(np.linalg.norm(out2 - out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae788b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.46 ms ± 311 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit conv_reference(Z, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b755e8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5 s ± 419 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit conv_naive(Z, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b784021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_mat_mul(Z, w):\n",
    "    N, H, W, C_IN = Z.shape\n",
    "    K, _, _, C_OUT = w.shape\n",
    "    new_h = new_w = get_height(H, K, 0, 1)\n",
    "    out = np.zeros((N, new_h, new_w, C_OUT))\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            out += Z[:, i:i+new_h, j:j+new_w,:] @ w[i, j]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "842e9ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.721059783274595e-13\n"
     ]
    }
   ],
   "source": [
    "out3 = conv_mat_mul(Z, w)\n",
    "print(np.linalg.norm(out3 - out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6830752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.52 ms ± 657 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit conv_mat_mul(Z, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "66545b59-a506-4709-a5cd-720ba9c740e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7b79019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 10])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> # NLP Example\n",
    ">>> batch, sentence_length, embedding_dim = 20, 5, 10\n",
    ">>> embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    ">>> layer_norm = nn.LayerNorm(embedding_dim)\n",
    ">>> # Activate module\n",
    ">>> layer_norm(embedding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f63e95e-89f2-474d-ba80-5f0d175f66ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad916a-add9-4ee8-ad9c-caf8a14c5786",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>\n",
    ">>> # Image Example\n",
    ">>> N, C, H, W = 20, 5, 10, 10\n",
    ">>> input = torch.randn(N, C, H, W)\n",
    ">>> # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\n",
    ">>> # as shown in the image below\n",
    ">>> layer_norm = nn.LayerNorm([C, H, W])\n",
    ">>> output = layer_norm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "996c8357-64d1-4a8a-bbfc-369cfee8e38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(range(10)).sum().reshape((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85e01a0b-6cbb-46a9-a0d4-f67772e49045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4000, 4.8000, 0.5500, 1.6500, 4.7000],\n",
       "        [0.2000, 3.5000, 3.5000, 1.1000, 0.2500],\n",
       "        [0.1000, 4.7500, 3.5500, 3.4000, 3.9000],\n",
       "        [1.7500, 4.6000, 4.5500, 1.3000, 4.5000],\n",
       "        [0.3000, 1.0000, 2.1500, 1.5500, 2.4500]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "shape = (5, 5)\n",
    "entropy = 1\n",
    "np.random.seed(np.prod(shape) * len(shape) * entropy)\n",
    "x = torch.tensor(np.random.randint(0, 100, size=shape) / 20, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da72117-86e2-4f17-9a80-51d6324bdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tiny_pytorch.Tensor([[ 0.13135943 -0.4084022   0.8505037   0.38247043 -0.23861545]\n",
    "  [ 0.01573963  0.05280813  0.9377031   0.15637057  0.36880913]\n",
    "  [-0.9809521  -0.37889847 -0.9718753  -0.7014624   0.9331728 ]\n",
    "  [ 0.95964324  0.46905363  0.509793   -0.08380664  0.9449951 ]\n",
    "  [-0.20502298  0.40138373  0.32844922  0.21638367 -0.6089812 ]]),\n",
    " tiny_pytorch.Tensor([[ 0.39952454  0.82827985  0.65007496 -0.14897561  0.9155253 ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc72509-a0cd-4a12-9bed-edc36057a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tiny_pytorch.Tensor([[ 0.6174666   0.49459407 -0.8219022   0.9120861  -0.24601288]\n",
    "  [-0.44928685  0.2557457  -0.07044538 -0.5360256   0.7427028 ]\n",
    "  [-0.70411    -0.5978629   0.35027173 -0.04575402 -0.9345301 ]\n",
    "  [-0.7909989  -0.8493924  -0.0486707   0.08828461  1.0035834 ]\n",
    "  [ 0.18357976  0.0584181   0.9251312   0.9185461  -0.5433214 ]]),\n",
    " tiny_pytorch.Tensor([[ 0.400128    1.017178   -0.59819704  0.47326997  0.6523597 ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "226126b5-c070-4012-99eb-a63d7b94f429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(*shape), nn.ReLU(), nn.Linear(*shape[::-1]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "30fed845-cab6-47b9-84e5-0e247d87472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].weight.data = torch.tensor([\n",
    "  [ 0.13135943, -0.4084022,   0.8505037, 0.38247043, -0.23861545],\n",
    "  [ 0.01573963, 0.05280813,  0.9377031, 0.15637057, 0.36880913],\n",
    "  [-0.9809521, -0.37889847, -0.9718753, -0.7014624, 0.9331728],\n",
    "  [ 0.95964324, 0.46905363, 0.509793, -0.08380664, 0.9449951],\n",
    "  [-0.20502298, 0.40138373, 0.32844922,  0.21638367, -0.6089812]\n",
    "])\n",
    "model[0].bias.data = torch.tensor([0.39952454, 0.82827985, 0.65007496, -0.14897561, 0.9155253])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "abb9abf9-20d1-43d8-b3ba-e864ff07bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[-1].weight.data = torch.tensor([\n",
    "  [ 0.6174666,   0.49459407, -0.8219022,   0.9120861,  -0.24601288],\n",
    "  [-0.44928685,  0.2557457,  -0.07044538, -0.5360256,   0.7427028 ],\n",
    "  [-0.70411,    -0.5978629,   0.35027173, -0.04575402, -0.9345301 ],\n",
    "  [-0.7909989,  -0.8493924,  -0.0486707,   0.08828461,  1.0035834 ],\n",
    "  [ 0.18357976, 0.0584181,   0.9251312,   0.9185461,  -0.5433214 ],\n",
    "])\n",
    "model[-1].bias.data = torch.tensor([ 0.400128,    1.017178,   -0.59819704,  0.47326997,  0.6523597 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "658b1bea-de05-46e4-a222-51ae3b4b2057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.7865,  1.9452,  0.0000,  0.0000, 13.8073],\n",
       "        [ 6.7287,  5.3090,  0.0000,  0.0000,  3.0065],\n",
       "        [10.9683,  4.3327,  0.0000,  0.0000, 10.7856],\n",
       "        [15.8921,  2.1745,  0.0000,  0.0000, 14.4926],\n",
       "        [ 7.2046,  0.7265,  0.0000,  0.0000,  6.8168]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ReLU()(model(x) + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "098607f5-4f19-4f37-96e5-c848fd52ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Sampler, BatchSampler, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ad473d4b-b270-4346-9a4f-20558c2bc4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m Dataset()\n",
       "\u001b[31mSource:\u001b[39m        \n",
       "\u001b[38;5;28;01mclass\u001b[39;00m Dataset(Generic[_T_co]):\n",
       "    \u001b[33mr\"\"\"An abstract class representing a :class:`Dataset`.\u001b[39m\n",
       "\n",
       "\u001b[33m    All datasets that represent a map from keys to data samples should subclass\u001b[39m\n",
       "\u001b[33m    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\u001b[39m\n",
       "\u001b[33m    data sample for a given key. Subclasses could also optionally overwrite\u001b[39m\n",
       "\u001b[33m    :meth:`__len__`, which is expected to return the size of the dataset by many\u001b[39m\n",
       "\u001b[33m    :class:`~torch.utils.data.Sampler` implementations and the default options\u001b[39m\n",
       "\u001b[33m    of :class:`~torch.utils.data.DataLoader`. Subclasses could also\u001b[39m\n",
       "\u001b[33m    optionally implement :meth:`__getitems__`, for speedup batched samples\u001b[39m\n",
       "\u001b[33m    loading. This method accepts list of indices of samples of batch and returns\u001b[39m\n",
       "\u001b[33m    list of samples.\u001b[39m\n",
       "\n",
       "\u001b[33m    .. note::\u001b[39m\n",
       "\u001b[33m      :class:`~torch.utils.data.DataLoader` by default constructs an index\u001b[39m\n",
       "\u001b[33m      sampler that yields integral indices.  To make it work with a map-style\u001b[39m\n",
       "\u001b[33m      dataset with non-integral indices/keys, a custom sampler must be provided.\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __getitem__(self, index) -> _T_co:\n",
       "        \u001b[38;5;28;01mraise\u001b[39;00m NotImplementedError(\u001b[33m\"Subclasses of Dataset should implement __getitem__.\"\u001b[39m)\n",
       "\n",
       "    \u001b[38;5;66;03m# def __getitems__(self, indices: List) -> List[_T_co]:\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# Not implemented to prevent false-positives in fetcher check in\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __add__(self, other: \u001b[33m\"Dataset[_T_co]\"\u001b[39m) -> \u001b[33m\"ConcatDataset[_T_co]\"\u001b[39m:\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m ConcatDataset([self, other])\n",
       "\n",
       "    \u001b[38;5;66;03m# No `def __len__(self)` default?\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# See NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# in pytorch/torch/utils/data/sampler.py\u001b[39;00m\n",
       "\u001b[31mFile:\u001b[39m           ~/dev/dl-venv/lib/python3.13/site-packages/torch/utils/data/dataset.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     IterableDataset, TensorDataset, StackDataset, ConcatDataset, Subset, MapDataPipe"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "44608546-e250-4aa4-8a8d-68089be17054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m Sampler(data_source: Optional[Sized] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mSource:\u001b[39m        \n",
       "\u001b[38;5;28;01mclass\u001b[39;00m Sampler(Generic[_T_co]):\n",
       "    \u001b[33mr\"\"\"Base class for all Samplers.\u001b[39m\n",
       "\n",
       "\u001b[33m    Every Sampler subclass has to provide an :meth:`__iter__` method, providing a\u001b[39m\n",
       "\u001b[33m    way to iterate over indices or lists of indices (batches) of dataset elements,\u001b[39m\n",
       "\u001b[33m    and may provide a :meth:`__len__` method that returns the length of the returned iterators.\u001b[39m\n",
       "\n",
       "\u001b[33m    Args:\u001b[39m\n",
       "\u001b[33m        data_source (Dataset): This argument is not used and will be removed in 2.2.0.\u001b[39m\n",
       "\u001b[33m            You may still have custom implementation that utilizes it.\u001b[39m\n",
       "\n",
       "\u001b[33m    Example:\u001b[39m\n",
       "\u001b[33m        >>> # xdoctest: +SKIP\u001b[39m\n",
       "\u001b[33m        >>> class AccedingSequenceLengthSampler(Sampler[int]):\u001b[39m\n",
       "\u001b[33m        >>>     def __init__(self, data: List[str]) -> None:\u001b[39m\n",
       "\u001b[33m        >>>         self.data = data\u001b[39m\n",
       "\u001b[33m        >>>\u001b[39m\n",
       "\u001b[33m        >>>     def __len__(self) -> int:\u001b[39m\n",
       "\u001b[33m        >>>         return len(self.data)\u001b[39m\n",
       "\u001b[33m        >>>\u001b[39m\n",
       "\u001b[33m        >>>     def __iter__(self) -> Iterator[int]:\u001b[39m\n",
       "\u001b[33m        >>>         sizes = torch.tensor([len(x) for x in self.data])\u001b[39m\n",
       "\u001b[33m        >>>         yield from torch.argsort(sizes).tolist()\u001b[39m\n",
       "\u001b[33m        >>>\u001b[39m\n",
       "\u001b[33m        >>> class AccedingSequenceLengthBatchSampler(Sampler[List[int]]):\u001b[39m\n",
       "\u001b[33m        >>>     def __init__(self, data: List[str], batch_size: int) -> None:\u001b[39m\n",
       "\u001b[33m        >>>         self.data = data\u001b[39m\n",
       "\u001b[33m        >>>         self.batch_size = batch_size\u001b[39m\n",
       "\u001b[33m        >>>\u001b[39m\n",
       "\u001b[33m        >>>     def __len__(self) -> int:\u001b[39m\n",
       "\u001b[33m        >>>         return (len(self.data) + self.batch_size - 1) // self.batch_size\u001b[39m\n",
       "\u001b[33m        >>>\u001b[39m\n",
       "\u001b[33m        >>>     def __iter__(self) -> Iterator[List[int]]:\u001b[39m\n",
       "\u001b[33m        >>>         sizes = torch.tensor([len(x) for x in self.data])\u001b[39m\n",
       "\u001b[33m        >>>         for batch in torch.chunk(torch.argsort(sizes), len(self)):\u001b[39m\n",
       "\u001b[33m        >>>             yield batch.tolist()\u001b[39m\n",
       "\n",
       "\u001b[33m    .. note:: The :meth:`__len__` method isn't strictly required by\u001b[39m\n",
       "\u001b[33m              :class:`~torch.utils.data.DataLoader`, but is expected in any\u001b[39m\n",
       "\u001b[33m              calculation involving the length of a :class:`~torch.utils.data.DataLoader`.\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(self, data_source: Optional[Sized] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "        \u001b[38;5;28;01mif\u001b[39;00m data_source \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "            \u001b[38;5;28;01mimport\u001b[39;00m warnings\n",
       "\n",
       "            warnings.warn(\n",
       "                \u001b[33m\"`data_source` argument is not used and will be removed in 2.2.0.\"\u001b[39m\n",
       "                \u001b[33m\"You may still have custom implementation that utilizes it.\"\u001b[39m\n",
       "            )\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __iter__(self) -> Iterator[_T_co]:\n",
       "        \u001b[38;5;28;01mraise\u001b[39;00m NotImplementedError\n",
       "\n",
       "    \u001b[38;5;66;03m# NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# Many times we have an abstract class representing a collection/iterable of\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# data, e.g., `torch.utils.data.Sampler`, with its subclasses optionally\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# implementing a `__len__` method. In such cases, we must make sure to not\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# provide a default implementation, because both straightforward default\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# implementations have their issues:\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#   + `return NotImplemented`:\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#     Calling `len(subclass_instance)` raises:\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#       TypeError: 'NotImplementedType' object cannot be interpreted as an integer\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#   + `raise NotImplementedError`:\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#     This prevents triggering some fallback behavior. E.g., the built-in\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#     `list(X)` tries to call `len(X)` first, and executes a different code\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#     path if the method is not found or `NotImplemented` is returned, while\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#     raising a `NotImplementedError` will propagate and make the call fail\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#     where it could have used `__iter__` to complete the call.\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#\u001b[39;00m\n",
       "    \u001b[38;5;66;03m# Thus, the only two sensible things to do are\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#   + **not** provide a default `__len__`.\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#   + raise a `TypeError` instead, which is what Python uses when users call\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#     a method that is not defined on an object.\u001b[39;00m\n",
       "    \u001b[38;5;66;03m#     (@ssnl verifies that this works on at least Python 3.7.)\u001b[39;00m\n",
       "\u001b[31mFile:\u001b[39m           ~/dev/dl-venv/lib/python3.13/site-packages/torch/utils/data/sampler.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     SequentialSampler, RandomSampler, SubsetRandomSampler, WeightedRandomSampler, BatchSampler, _InfiniteConstantSampler, DistributedSampler"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sampler??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c67931-7e70-4f65-b2c7-39634ce2e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "65690ac22ab795aa8632198c42e65c403586a4ca9873bfeb1584bacbad91cd4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
